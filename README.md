# Credit-card-fraud-detection
Summary



In this era of digitalisation, credit card fraud have been seeing a steady rise over the years. This has led to financial institutions & business to deploy models & techniques that help to identify thefts in order to keep the losses at the minimum. Here,the dataset has been imported from kaggle, to see the probability of fraudulent credit card detection,using features like basic information of the credit card holder,the category the merchant belongs to, but most importantly the amount transacted, the transaction date & time which are of prime importance to identify patterns that help in seperating the fraudulent from the normal transactions at large. We start with removing the outliers for the variable amount transacted by the credit card holder.This helps us in removing unsual transactions amounts by finding patterns in the data,that fall outside the given range and that do not conform to expected behaviour.

The datatset shows a total of 18,52,394 transactions out of which 9651 are frauduent and 18,42,743 are normal transactions from 1st January 2019 to 31st December 2020. The next step would be visualisation of data, known as Exploratory data analysis.

1) Through the figure plotted by importing the map of United States, we see that fraudulent transaction of the credit card holders as well as the merchants, are widely present in states of US, with comparitively higher fraudulent transactions in most of the states given the exception of lower fraudulent transactions taking place in Alaska and Honolulu. 2) Further the data leads us to the fact where we can see that fraudulent transactions is higher at late night around 0.40 per cent as compared to other time periods. 3) The graph show that fraudulent transaction is highest for the month of March, January, May, June,August & October. 4) The graph show that fraudulent transaction is highest on Saturdays, while the normal transactions is highest on Mondays. 5) Here we see that categories like grocery point of sale, miscellaneous net,shopping net, shopping point of sale,entertainment and home show only normal transactions, while gas transport,food dining, kids pet,personal care, health & fitness, grocery net & travel show presence of fraudulent transactions. 6) Here we see people 60+ years of age, show highest number of fraudulent transactions. 7) The fraudulent transactions is higher for females then for males. 8) The countplot below showcases the fact that it is the medium sized population that shows the highest number of fradulent transactions at large. The above gave me a certain idea about how the fraudulent transactions could be distinguished from normal transactions purely based on visualising and exploring the independent variables at large.

The step that follows after includes removing unnecessary independent variables,encoding the variables which automatically assigns numbers to them for easy computation of the model, standarising the data, splitting the dataset into training and testing in the ratio of 80:20 & lastly doing random under and over sampling to balance the data as it is a highly imbalanced dataset. Training a model means to train an algorithm to predict the outcome using models like LGBMClassifier, Decision Tree Classifier, Random Forest Classifier, Linear Discriminant Analysis, Logistic Regression and Gaussian Naive Bayes Classifier, while the 20 per cent data kept aside is to validate the model built by measuring the accuracy of the algorithm you are using to train the machine through ROC score, confusion matrix , test accuracy score alongside the classification report which gives us the precision & recall value.

a) Confusion matrix a 2*2 matrix, helps us identify correct predictions of a model as well as errors for the different individual classes in order to evaluate the model. i) True Negative- The transaction being predicted as normal is in actual also a normal transaction. ii)False Negative- It tells us how many samples from the dataset pedicted as Normal transactions turn out to be Fraudulent transactions in actual. iii)False Positive- From the total number of transactions which have been predicted as Fraud, how many od them are normal transactions in reality. iv)True Positive- The transaction being predicted as fraud is in actual also a fraudulent transaction.

b) Recall- This tells us how often the actual fraudulent transaction captured by our model had been correctly predicted as a fraudulent transaction. c) Precision- This tells us that when a model predicts a fraudulent transaction how often is it correct.A high precision rate indicates a low false positive rate. In our case Recall value holds more importance because if a fraudulent transaction is predicted as normal transaction then the consequence will be bad.

d)ROC- It is a trade off between True Positive Rate & False Positive Rate. Higher is the ROC value more better is the model at distinguishing between the fraudulent and non-fraudulent transactions. Test Accuracy- How accurately the model used will be able to predict the number of fraudulent transaction from the entire dataset thus helping us to select the best model for our dataset.

Once the above parameters have been understood, we fit our dataset into the various models given to see which gives us the best result.After running all the models mentioned above we reach the conclusion that a balanced LGBM Classifier is the best model as it fulfills the above criteria with test accuracy of 96 per cent, ROC and recall value of 90 per cent. Here the test accuracy and ROC being the highest from amongst all model indicate LGBM classifier to be the best choice for predicting fraudulent transactions at large.Also a recall value of 90 per cent means that around 90 per cent of the actual fraudulent transactions have been correctly idenitified by the classifier.
